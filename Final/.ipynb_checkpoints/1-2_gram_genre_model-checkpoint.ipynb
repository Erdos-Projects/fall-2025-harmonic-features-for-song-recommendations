{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e8ea9c5-c04d-4ce5-9e90-6f3304acd446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b61f776-4442-49b4-9ea2-ce1b74cc374e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joshua\\Documents\\GitHub\\fall-2025-harmonic-features-for-song-recommendations\\chord_matrix_conversion\\final\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c032917b-bddd-4151-9d50-eecd1b0acf83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/chord_and_genre.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load the chord and genre dataframe\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Note: this file is too big to store in GitHub, ~575+ Kb\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m chord_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/chord_and_genre.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/chord_and_genre.csv'"
     ]
    }
   ],
   "source": [
    "# load the chord and genre dataframe\n",
    "# Note: this file is too big to store in GitHub, ~575+ Kb\n",
    "chord_data = pd.read_csv('../data/chord_and_genre.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e1c9d6-378d-4212-ad03-1fc40eeaafd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the genre deviations dataframe\n",
    "n = 2\n",
    "filepath = '../data/' + str(n) + '_gram_deviations.csv'\n",
    "deviation_df = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70955658-aa38-475a-a252-edfd4b1220f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the sample size threshold\n",
    "###################################################################################################################\n",
    "sample_size_threshold = 0.01 # make this bigger to exclude rarer chords from the model\n",
    "###################################################################################################################\n",
    "baseline_total = deviation_df.loc[deviation_df['n_gram'] == 'baseline', 'total'].iloc[0]\n",
    "\n",
    "# drop the baseline row\n",
    "deviation_df = deviation_df[deviation_df['n_gram'] != 'baseline']\n",
    "\n",
    "# drop all rows not meeting a sample size threshold\n",
    "print(\"Number of n-grams before dropping based on sample size threshold:\", len(deviation_df.index))\n",
    "deviation_df = deviation_df[deviation_df['total'] >= baseline_total * sample_size_threshold]\n",
    "print(\"Number of n-grams after dropping based on sample size threshold:\", len(deviation_df.index))\n",
    "\n",
    "# sort by maximum absolute log deviation ration, so that the \"good feature\" chords are at the top\n",
    "deviation_df = deviation_df.sort_values(by = 'max_abs_log_dev_ratio', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a575ac-328c-4090-adf6-de74ab95b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the equivalence dictionary file\n",
    "# this is a dictionary of dictionaries\n",
    "#    the top-level keys are chord names (e.g. 'C','Amin')\n",
    "#    the top-level values are dictionaries, whose keys are equivalent chords, and whose values are the semitone distance between the top-level key and the low-level key\n",
    "with open('../data/harmonic_equivalence_dictionary.json') as file:\n",
    "    equiv_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b8a054-007a-4337-98d7-83eafe3f857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the two input chords are harmonically equivalent, return (True, num_semitones) where num_semitones is the distance from n_gram_1 (up) to n_gram_2\n",
    "# otherwise, return (False, None)\n",
    "def compare_chords(chord_1, chord_2):\n",
    "    if chord_2 in equiv_dict[chord_1]:\n",
    "        return (True, equiv_dict[chord_1][chord_2])\n",
    "    else:\n",
    "        return (False, None)\n",
    "\n",
    "assert(compare_chords('C','D')[0])\n",
    "assert(compare_chords('C','E')[0])\n",
    "assert(not(compare_chords('C','Amin')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f12851-f7a1-4b21-8f54-a655d3c509b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the two input n_grams are harmonically equivalent, return (True, num_semitones) where num_semitones is the distance from n_gram_1 (up) to n_gram_2\n",
    "# otherwise, return (False, None)\n",
    "def compare_n_grams(n_gram_1, n_gram_2):\n",
    "    list_1 = n_gram_1.split(',')\n",
    "    list_2 = n_gram_2.split(',')\n",
    "\n",
    "    # if they aren't the same length, we don't have to check anything\n",
    "    if len(list_1) != len(list_2):\n",
    "        return (False, None)\n",
    "\n",
    "    # now we can assume they have the same length\n",
    "    comparison = [compare_chords(list_1[i], list_2[i]) for i in range(len(list_1))]\n",
    "\n",
    "    # if any pairs are not the same, return False\n",
    "    for c in comparison:\n",
    "        if not c[0]:\n",
    "            return (False, None)\n",
    "\n",
    "    # now we can assume every respective pair is equivalent, but we still need all of the distances to match\n",
    "    dist_0 = comparison[0][1]\n",
    "    for c in comparison:\n",
    "        if c[1] != dist_0:\n",
    "            return (False, None)\n",
    "\n",
    "    return (True, dist_0)\n",
    "\n",
    "assert(compare_n_grams('C,D,E','F,G,A')[0])\n",
    "assert(not(compare_n_grams('C,D,E','F,G,B')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd451e0-0c5f-4b06-98b0-874bf307d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return true/false depending on if a song contains a harmonically equivalent n_gram to the input n_gram\n",
    "# new version of this, making use of the equivalence dictionary for lookups rather than doing calculations every time\n",
    "def contains_n_gram(song, n_gram):\n",
    "    # assumption: input song is a comma-separated string of chord names\n",
    "    # assumption: input n_gram is a comma-separated string of chord names\n",
    "\n",
    "    # skip ahead and return true if the raw version is the song\n",
    "    if n_gram in song:\n",
    "        return True\n",
    "\n",
    "    # split up the song and n_gram into lists of strings of single chords\n",
    "    song_as_list = song.split(',')\n",
    "    song_length = len(song_as_list)\n",
    "    n_gram_as_list = n_gram.split(',')\n",
    "    n = len(n_gram_as_list)\n",
    "\n",
    "    for i in range(0,song_length - n):\n",
    "        song_n_gram = ','.join(song_as_list[i:i+n])\n",
    "        is_same, dist = compare_n_grams(n_gram, song_n_gram)\n",
    "        if is_same:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "assert(contains_n_gram('A,B,C,D,E,F,G','C,D'))\n",
    "assert(contains_n_gram('A,B,C,D,E,F','F,G'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d380bd44-7246-40fe-b86a-2979d0adb979",
   "metadata": {},
   "source": [
    "With the above setup out of the way, now I want to make a classifier model which will output a genre prediction. The only features will be a series of binary columns of the form 'Contains a 2-gram harmonically equivalent to C,D' etc. To decide which chords to use, I'm using the top sorted chords from the deviation dataframe loaded above.\n",
    "\n",
    "I will also make two different baslines to compare against:\n",
    "\n",
    "    1. Predict most common, i.e. just predict 'pop' for every song.\n",
    "    \n",
    "    2. An alternate classifier of similar type, whose inputs are also binary columns, but of the form 'Contains a (literal, raw) C,D 2-gram'. In other words, the same kinds of features, but without considering harmonic equivalence. I'll have it use the same columns as the harmonic-equivalence-based classifier, unless I think of a better way to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff4b03-0cbd-4a07-b340-2f0a06e4fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d6303d-96cf-423e-9d5f-4fbd0a29f859",
   "metadata": {},
   "source": [
    "Run cells from here down and change num_feature_chords to make and test a mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88b8c8d-5757-417c-aff9-e900bb2b76d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a one-hot column for a list of feature chords to the chord and genre data\n",
    "def add_one_hot(feature_chords):\n",
    "    df = pd.read_csv('chord_and_genre.csv')\n",
    "\n",
    "    new_feature_chords = [fc for fc in feature_chords if (('has_literal_'+fc) not in list(df.columns))]\n",
    "    num_new_features = len(new_feature_chords) \n",
    "\n",
    "    if num_new_features == 0:\n",
    "        print(\"No new feature columns to build.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Building \" + str(num_new_features) + \" one-hot encoded columns for chord containment.\\n\")\n",
    "    t0 = time.time()\n",
    "    for index, fc in enumerate(new_feature_chords):\n",
    "        df['has_literal_' + fc] = df['chords'].apply(lambda song : fc in song)\n",
    "        df['has_equivalent_' + fc] = df['chords'].apply(lambda song : contains_n_gram(song, fc))\n",
    "        \n",
    "        print(\"Finished tabulating columns for:\",fc)\n",
    "        print(\"\\tCompleted chords so far:\",index+1)\n",
    "        print(\"\\tChords remaining:\",num_new_features - (index+1))\n",
    "        print(\"\\tAverage time per chord so far:\",np.round((time.time()-t0)/(index+1), decimals=1))\n",
    "        print()\n",
    "\n",
    "    print(\"Finished all tabulations.\")\n",
    "    df.to_csv('chord_and_genre.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f7cd4-17d8-4bab-9f19-f9821a104888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract \"good\" feature chords from the top rows of the deviation dataframe\n",
    "features_to_try = 60\n",
    "features_to_try = min(features_to_try, len(deviation_df.index)) # causes an error if you try to do more than the number of rows in the dataframe\n",
    "top_rows = deviation_df.head(features_to_try)\n",
    "feature_chords = list(top_rows['n_gram'])\n",
    "\n",
    "# add one-hot columns to the chord and genre dataframe\n",
    "add_one_hot(feature_chords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e15d5c-3644-4456-8b6a-3336ce92f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the feature dataframes\n",
    "model_df = pd.read_csv('chord_and_genre.csv')\n",
    "\n",
    "# make a numerically encoded genre column\n",
    "encoder = LabelEncoder()\n",
    "genre_encoded = encoder.fit_transform(model_df['genres'])\n",
    "\n",
    "# make some convenient handles\n",
    "literal_columns = ['has_literal_' + fc for fc in feature_chords]\n",
    "equivalent_columns = ['has_equivalent_' + fc for fc in feature_chords]\n",
    "feature_columns = literal_columns + equivalent_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e6b54-2244-4c1f-b5c6-bf6a177e47ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6190268-fe8b-465c-abaf-fc6583d82e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a train test split on the harmonic equivalence dataframe\n",
    "X_train, X_test, y_train, y_test = train_test_split(model_df[feature_columns], \n",
    "                                                    genre_encoded,\n",
    "                                                    random_state = 145,\n",
    "                                                    test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8dff09-d552-40e3-8362-2ea6cacc8d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the models and make predictions\n",
    "equivalence_model = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs', max_iter = 2000)\n",
    "equivalence_model.fit(X_train[equivalent_columns], y_train)\n",
    "y_pred_equiv = equivalence_model.predict(X_test[equivalent_columns])\n",
    "\n",
    "literal_model = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs', max_iter = 2000)\n",
    "literal_model.fit(X_train[literal_columns], y_train)\n",
    "y_pred_lit = literal_model.predict(X_test[literal_columns])\n",
    "\n",
    "dummy_model = DummyClassifier(strategy = 'most_frequent')\n",
    "dummy_model.fit(X_train, y_train)\n",
    "y_pred_dummy = dummy_model.predict(X_test)\n",
    "\n",
    "# compute and output accuracy scores\n",
    "accuracy_equiv = accuracy_score(y_test, y_pred_equiv)\n",
    "print(\"Accuracy of equivalence model:\\t\", accuracy_equiv)\n",
    "accuracy_lit = accuracy_score(y_test, y_pred_lit)\n",
    "print(\"Accuracy of literal model:\\t\", accuracy_lit)\n",
    "accuracy_dummy = accuracy_score(y_test, y_pred_dummy)\n",
    "print(\"Accuracy of dummy model:\\t\", accuracy_dummy)\n",
    "print()\n",
    "\n",
    "# compute and output precision scores\n",
    "precision_equiv = precision_score(y_test, y_pred_equiv, average = 'micro')\n",
    "print(\"Precision of equivalence model:\\t\", precision_equiv)\n",
    "precision_lit = precision_score(y_test, y_pred_lit, average = 'micro')\n",
    "print(\"Precision of literal model:\\t\", precision_lit)\n",
    "precision_dummy = precision_score(y_test, y_pred_dummy, average = 'micro')\n",
    "print(\"Precision of dummy model:\\t\", precision_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba87f0-f7c0-48b5-afcc-2365dbda5ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(\n",
    "    max_depth = 10,\n",
    "    min_samples_leaf = 5,\n",
    "    random_state = 145)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators = 500,\n",
    "    max_depth = 10,\n",
    "    min_samples_leaf = 5,\n",
    "    #max_features = 2,\n",
    "    bootstrap = True,\n",
    "    max_samples = 500,\n",
    "    random_state = 145)\n",
    "\n",
    "et = ExtraTreesClassifier(\n",
    "    n_estimators = 500,\n",
    "    max_depth = 10,\n",
    "    min_samples_leaf = 5,\n",
    "    #max_features = 2,\n",
    "    #bootstrap = True,\n",
    "    #max_samples = 500,\n",
    "    random_state = 145)\n",
    "\n",
    "tree.fit(X_train[equivalent_columns], y_train)\n",
    "rf.fit(X_train[equivalent_columns], y_train)\n",
    "et.fit(X_train[equivalent_columns], y_train)\n",
    "\n",
    "tree_pred = tree.predict(X_test[equivalent_columns])\n",
    "rf_pred = rf.predict(X_test[equivalent_columns])\n",
    "et_pred = et.predict(X_test[equivalent_columns])\n",
    "\n",
    "accuracy_tree = accuracy_score(y_test, tree_pred)\n",
    "print(\"Accuracy of decision tree model:\\t\", accuracy_tree)\n",
    "accuracy_rf = accuracy_score(y_test, rf_pred)\n",
    "print(\"Accuracy of random forest model:\\t\", accuracy_rf)\n",
    "accuracy_et = accuracy_score(y_test, et_pred)\n",
    "print(\"Accuracy of extra trees model:\\t\\t\", accuracy_et)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
