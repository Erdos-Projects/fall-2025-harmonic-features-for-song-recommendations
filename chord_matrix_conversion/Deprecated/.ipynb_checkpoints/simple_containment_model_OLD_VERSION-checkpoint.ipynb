{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e8ea9c5-c04d-4ce5-9e90-6f3304acd446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c032917b-bddd-4151-9d50-eecd1b0acf83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the chord and genre dataframe\n",
    "# Note: this file is too big to store in GitHub, ~575+ Kb\n",
    "chord_data = pd.read_csv('../../data/chord_and_genre.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e1c9d6-378d-4212-ad03-1fc40eeaafd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the genre deviations dataframe\n",
    "n = 2\n",
    "filepath = '../data/' + str(n) + '_gram_deviations.csv'\n",
    "deviation_df = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70955658-aa38-475a-a252-edfd4b1220f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the sample size threshold\n",
    "###################################################################################################################\n",
    "sample_size_threshold = 0.01 # make this bigger to exclude rarer chords from the model\n",
    "###################################################################################################################\n",
    "baseline_total = deviation_df.loc[deviation_df['n_gram'] == 'baseline', 'total'].iloc[0]\n",
    "\n",
    "# drop the baseline row\n",
    "deviation_df = deviation_df[deviation_df['n_gram'] != 'baseline']\n",
    "\n",
    "# drop all rows not meeting a sample size threshold\n",
    "print(\"Number of n-grams before dropping based on sample size threshold:\", len(deviation_df.index))\n",
    "deviation_df = deviation_df[deviation_df['total'] >= baseline_total * sample_size_threshold]\n",
    "print(\"Number of n-grams after dropping based on sample size threshold:\", len(deviation_df.index))\n",
    "\n",
    "# sort by maximum absolute log deviation ration, so that the \"good feature\" chords are at the top\n",
    "deviation_df = deviation_df.sort_values(by = 'max_abs_log_dev_ratio', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a575ac-328c-4090-adf6-de74ab95b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the equivalence dictionary file\n",
    "# this is a dictionary of dictionaries\n",
    "#    the top-level keys are chord names (e.g. 'C','Amin')\n",
    "#    the top-level values are dictionaries, whose keys are equivalent chords, and whose values are the semitone distance between the top-level key and the low-level key\n",
    "with open('../data/harmonic_equivalence_dictionary.json') as file:\n",
    "    equiv_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b8a054-007a-4337-98d7-83eafe3f857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the two input chords are harmonically equivalent, return (True, num_semitones) where num_semitones is the distance from n_gram_1 (up) to n_gram_2\n",
    "# otherwise, return (False, None)\n",
    "def compare_chords(chord_1, chord_2):\n",
    "    if chord_2 in equiv_dict[chord_1]:\n",
    "        return (True, equiv_dict[chord_1][chord_2])\n",
    "    else:\n",
    "        return (False, None)\n",
    "\n",
    "assert(compare_chords('C','D')[0])\n",
    "assert(compare_chords('C','E')[0])\n",
    "assert(not(compare_chords('C','Amin')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f12851-f7a1-4b21-8f54-a655d3c509b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the two input n_grams are harmonically equivalent, return (True, num_semitones) where num_semitones is the distance from n_gram_1 (up) to n_gram_2\n",
    "# otherwise, return (False, None)\n",
    "def compare_n_grams(n_gram_1, n_gram_2):\n",
    "    list_1 = n_gram_1.split(',')\n",
    "    list_2 = n_gram_2.split(',')\n",
    "\n",
    "    # if they aren't the same length, we don't have to check anything\n",
    "    if len(list_1) != len(list_2):\n",
    "        return (False, None)\n",
    "\n",
    "    # now we can assume they have the same length\n",
    "    comparison = [compare_chords(list_1[i], list_2[i]) for i in range(len(list_1))]\n",
    "\n",
    "    # if any pairs are not the same, return False\n",
    "    for c in comparison:\n",
    "        if not c[0]:\n",
    "            return (False, None)\n",
    "\n",
    "    # now we can assume every respective pair is equivalent, but we still need all of the distances to match\n",
    "    dist_0 = comparison[0][1]\n",
    "    for c in comparison:\n",
    "        if c[1] != dist_0:\n",
    "            return (False, None)\n",
    "\n",
    "    return (True, dist_0)\n",
    "\n",
    "assert(compare_n_grams('C,D,E','F,G,A')[0])\n",
    "assert(not(compare_n_grams('C,D,E','F,G,B')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd451e0-0c5f-4b06-98b0-874bf307d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return true/false depending on if a song contains a harmonically equivalent n_gram to the input n_gram\n",
    "# new version of this, making use of the equivalence dictionary for lookups rather than doing calculations every time\n",
    "def contains_n_gram(song, n_gram):\n",
    "    # assumption: input song is a comma-separated string of chord names\n",
    "    # assumption: input n_gram is a comma-separated string of chord names\n",
    "\n",
    "    # skip ahead and return true if the raw version is the song\n",
    "    if n_gram in song:\n",
    "        return True\n",
    "\n",
    "    # split up the song and n_gram into lists of strings of single chords\n",
    "    song_as_list = song.split(',')\n",
    "    song_length = len(song_as_list)\n",
    "    n_gram_as_list = n_gram.split(',')\n",
    "    n = len(n_gram_as_list)\n",
    "\n",
    "    for i in range(0,song_length - n):\n",
    "        song_n_gram = ','.join(song_as_list[i:i+n])\n",
    "        is_same, dist = compare_n_grams(n_gram, song_n_gram)\n",
    "        if is_same:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "assert(contains_n_gram('A,B,C,D,E,F,G','C,D'))\n",
    "assert(contains_n_gram('A,B,C,D,E,F','F,G'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d380bd44-7246-40fe-b86a-2979d0adb979",
   "metadata": {},
   "source": [
    "With the above setup out of the way, now I want to make a classifier model which will output a genre prediction. The only features will be a series of binary columns of the form 'Contains a 2-gram harmonically equivalent to C,D' etc. To decide which chords to use, I'm using the top sorted chords from the deviation dataframe loaded above.\n",
    "\n",
    "I will also make two different baslines to compare against:\n",
    "\n",
    "    1. Predict most common, i.e. just predict 'pop' for every song.\n",
    "    \n",
    "    2. An alternate classifier of similar type, whose inputs are also binary columns, but of the form 'Contains a (literal, raw) C,D 2-gram'. In other words, the same kinds of features, but without considering harmonic equivalence. I'll have it use the same columns as the harmonic-equivalence-based classifier, unless I think of a better way to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff4b03-0cbd-4a07-b340-2f0a06e4fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d6303d-96cf-423e-9d5f-4fbd0a29f859",
   "metadata": {},
   "source": [
    "Run cells from here down and change num_feature_chords to make and test a mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88b8c8d-5757-417c-aff9-e900bb2b76d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a one-hot column for a list of feature chords to the chord and genre data\n",
    "def add_one_hot(feature_chords):\n",
    "    df = pd.read_csv('chord_and_genre.csv')\n",
    "\n",
    "    new_feature_chords = [fc for fc in feature_chords if (('has_literal_'+fc) not in list(df.columns))]\n",
    "    num_new_features = len(new_feature_chords) \n",
    "\n",
    "    if num_new_features == 0:\n",
    "        print(\"No new feature columns to build.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Building \" + str(num_new_features) + \" one-hot encoded columns for chord containment.\\n\")\n",
    "    t0 = time.time()\n",
    "    for index, fc in enumerate(new_feature_chords):\n",
    "        df['has_literal_' + fc] = df['chords'].apply(lambda song : fc in song)\n",
    "        df['has_equivalent_' + fc] = df['chords'].apply(lambda song : contains_n_gram(song, fc))\n",
    "        \n",
    "        print(\"Finished tabulating columns for:\",fc)\n",
    "        print(\"\\tCompleted chords so far:\",index+1)\n",
    "        print(\"\\tChords remaining:\",num_new_features - (index+1))\n",
    "        print(\"\\tAverage time per chord so far:\",np.round((time.time()-t0)/(index+1), decimals=1))\n",
    "        print()\n",
    "\n",
    "    print(\"Finished all tabulations.\")\n",
    "    df.to_csv('chord_and_genre.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f7cd4-17d8-4bab-9f19-f9821a104888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract \"good\" feature chords from the top rows of the deviation dataframe\n",
    "features_to_try = 60\n",
    "features_to_try = min(features_to_try, len(deviation_df.index)) # causes an error if you try to do more than the number of rows in the dataframe\n",
    "top_rows = deviation_df.head(features_to_try)\n",
    "feature_chords = list(top_rows['n_gram'])\n",
    "\n",
    "# add one-hot columns to the chord and genre dataframe\n",
    "add_one_hot(feature_chords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e15d5c-3644-4456-8b6a-3336ce92f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the feature dataframes\n",
    "model_df = pd.read_csv('chord_and_genre.csv')\n",
    "\n",
    "# make a numerically encoded genre column\n",
    "encoder = LabelEncoder()\n",
    "genre_encoded = encoder.fit_transform(model_df['genres'])\n",
    "\n",
    "# make some convenient handles\n",
    "literal_columns = ['has_literal_' + fc for fc in feature_chords]\n",
    "equivalent_columns = ['has_equivalent_' + fc for fc in feature_chords]\n",
    "feature_columns = literal_columns + equivalent_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e6b54-2244-4c1f-b5c6-bf6a177e47ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6190268-fe8b-465c-abaf-fc6583d82e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a train test split on the harmonic equivalence dataframe\n",
    "X_train, X_test, y_train, y_test = train_test_split(model_df[feature_columns], \n",
    "                                                    genre_encoded,\n",
    "                                                    random_state = 145,\n",
    "                                                    test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8dff09-d552-40e3-8362-2ea6cacc8d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the models and make predictions\n",
    "equivalence_model = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs', max_iter = 2000)\n",
    "equivalence_model.fit(X_train[equivalent_columns], y_train)\n",
    "y_pred_equiv = equivalence_model.predict(X_test[equivalent_columns])\n",
    "\n",
    "literal_model = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs', max_iter = 2000)\n",
    "literal_model.fit(X_train[literal_columns], y_train)\n",
    "y_pred_lit = literal_model.predict(X_test[literal_columns])\n",
    "\n",
    "dummy_model = DummyClassifier(strategy = 'most_frequent')\n",
    "dummy_model.fit(X_train, y_train)\n",
    "y_pred_dummy = dummy_model.predict(X_test)\n",
    "\n",
    "# compute and output accuracy scores\n",
    "accuracy_equiv = accuracy_score(y_test, y_pred_equiv)\n",
    "print(\"Accuracy of equivalence model:\\t\", accuracy_equiv)\n",
    "accuracy_lit = accuracy_score(y_test, y_pred_lit)\n",
    "print(\"Accuracy of literal model:\\t\", accuracy_lit)\n",
    "accuracy_dummy = accuracy_score(y_test, y_pred_dummy)\n",
    "print(\"Accuracy of dummy model:\\t\", accuracy_dummy)\n",
    "print()\n",
    "\n",
    "# compute and output precision scores\n",
    "precision_equiv = precision_score(y_test, y_pred_equiv, average = 'micro')\n",
    "print(\"Precision of equivalence model:\\t\", precision_equiv)\n",
    "precision_lit = precision_score(y_test, y_pred_lit, average = 'micro')\n",
    "print(\"Precision of literal model:\\t\", precision_lit)\n",
    "precision_dummy = precision_score(y_test, y_pred_dummy, average = 'micro')\n",
    "print(\"Precision of dummy model:\\t\", precision_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba87f0-f7c0-48b5-afcc-2365dbda5ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(\n",
    "    max_depth = 10,\n",
    "    min_samples_leaf = 5,\n",
    "    random_state = 145)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators = 500,\n",
    "    max_depth = 10,\n",
    "    min_samples_leaf = 5,\n",
    "    #max_features = 2,\n",
    "    bootstrap = True,\n",
    "    max_samples = 500,\n",
    "    random_state = 145)\n",
    "\n",
    "et = ExtraTreesClassifier(\n",
    "    n_estimators = 500,\n",
    "    max_depth = 10,\n",
    "    min_samples_leaf = 5,\n",
    "    #max_features = 2,\n",
    "    #bootstrap = True,\n",
    "    #max_samples = 500,\n",
    "    random_state = 145)\n",
    "\n",
    "tree.fit(X_train[equivalent_columns], y_train)\n",
    "rf.fit(X_train[equivalent_columns], y_train)\n",
    "et.fit(X_train[equivalent_columns], y_train)\n",
    "\n",
    "tree_pred = tree.predict(X_test[equivalent_columns])\n",
    "rf_pred = rf.predict(X_test[equivalent_columns])\n",
    "et_pred = et.predict(X_test[equivalent_columns])\n",
    "\n",
    "accuracy_tree = accuracy_score(y_test, tree_pred)\n",
    "print(\"Accuracy of decision tree model:\\t\", accuracy_tree)\n",
    "accuracy_rf = accuracy_score(y_test, rf_pred)\n",
    "print(\"Accuracy of random forest model:\\t\", accuracy_rf)\n",
    "accuracy_et = accuracy_score(y_test, et_pred)\n",
    "print(\"Accuracy of extra trees model:\\t\\t\", accuracy_et)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
