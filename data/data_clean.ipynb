{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41d95f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ae26d",
   "metadata": {},
   "source": [
    "Read csv and standardize NaNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0376ee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv imported fom https://huggingface.co/datasets/ailsntua/Chordonomicon/blob/main/chordonomicon_v2.csv\n",
    "raw_df=pd.read_csv('../../data/chordonomicon_raw.csv', low_memory=False)\n",
    "raw_df = raw_df.replace({np.nan: pd.NA})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fab7965",
   "metadata": {},
   "source": [
    "Since we are analyzing release dates, main genre and popularity (via spotify song id), remove any entries that don't have any of these features. In the end, it seems that only 70% of the data could be useful to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ecdea62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301564, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>chords</th>\n",
       "      <th>release_date</th>\n",
       "      <th>genres</th>\n",
       "      <th>decade</th>\n",
       "      <th>rock_genre</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>main_genre</th>\n",
       "      <th>spotify_song_id</th>\n",
       "      <th>spotify_artist_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>&lt;intro_1&gt; E D A/Cs E D A/Cs &lt;verse_1&gt; E D A/Cs...</td>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>'alternative metal' 'alternative rock' 'nu met...</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>pop rock</td>\n",
       "      <td>artist_2</td>\n",
       "      <td>metal</td>\n",
       "      <td>2ffJZ2r8HxI5DHcmf3BO6c</td>\n",
       "      <td>694QW15WkebjcrWgQHzRYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>&lt;intro_1&gt; Csmin &lt;verse_1&gt; A Csmin A Csmin A Cs...</td>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>'alternative metal' 'canadian rock' 'funk meta...</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>canadian rock</td>\n",
       "      <td>artist_3</td>\n",
       "      <td>metal</td>\n",
       "      <td>5KiY8SZEnvCPyIEkFGRR3y</td>\n",
       "      <td>0niJkG4tKkne3zwr7I8n9n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>&lt;intro_1&gt; C &lt;verse_1&gt; G C G C &lt;chorus_1&gt; F Dmi...</td>\n",
       "      <td>2023-02-10</td>\n",
       "      <td>'modern country pop'</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>artist_5</td>\n",
       "      <td>pop</td>\n",
       "      <td>3zUecdrWC3IqrNSjhnoF3G</td>\n",
       "      <td>4GGfAshSkqoxpZdoaHm7ky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>&lt;intro_1&gt; G Bmin Amin D G Bmin &lt;verse_1&gt; Amin ...</td>\n",
       "      <td>2023-02-10</td>\n",
       "      <td>'modern country pop'</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>artist_5</td>\n",
       "      <td>pop</td>\n",
       "      <td>1gh9q0HsS3tVXQypDXp4gf</td>\n",
       "      <td>4GGfAshSkqoxpZdoaHm7ky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>&lt;intro_1&gt; Fsmin Fsno3d Bno3d E/B Fsno3d Bno3d ...</td>\n",
       "      <td>2023-02-10</td>\n",
       "      <td>'french pop' 'nouvelle chanson francaise'</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>artist_7</td>\n",
       "      <td>pop</td>\n",
       "      <td>4y3uAOMHISJ3OOdjPC1FFN</td>\n",
       "      <td>6e3pZKXUxrPfnUPJ960Hd9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             chords release_date  \\\n",
       "1   2  <intro_1> E D A/Cs E D A/Cs <verse_1> E D A/Cs...   2003-01-01   \n",
       "2   3  <intro_1> Csmin <verse_1> A Csmin A Csmin A Cs...   2003-01-01   \n",
       "4   5  <intro_1> C <verse_1> G C G C <chorus_1> F Dmi...   2023-02-10   \n",
       "6   7  <intro_1> G Bmin Amin D G Bmin <verse_1> Amin ...   2023-02-10   \n",
       "7   8  <intro_1> Fsmin Fsno3d Bno3d E/B Fsno3d Bno3d ...   2023-02-10   \n",
       "\n",
       "                                              genres  decade     rock_genre  \\\n",
       "1  'alternative metal' 'alternative rock' 'nu met...  2000.0       pop rock   \n",
       "2  'alternative metal' 'canadian rock' 'funk meta...  2000.0  canadian rock   \n",
       "4                               'modern country pop'  2020.0           <NA>   \n",
       "6                               'modern country pop'  2020.0           <NA>   \n",
       "7          'french pop' 'nouvelle chanson francaise'  2020.0           <NA>   \n",
       "\n",
       "  artist_id main_genre         spotify_song_id       spotify_artist_id  \n",
       "1  artist_2      metal  2ffJZ2r8HxI5DHcmf3BO6c  694QW15WkebjcrWgQHzRYF  \n",
       "2  artist_3      metal  5KiY8SZEnvCPyIEkFGRR3y  0niJkG4tKkne3zwr7I8n9n  \n",
       "4  artist_5        pop  3zUecdrWC3IqrNSjhnoF3G  4GGfAshSkqoxpZdoaHm7ky  \n",
       "6  artist_5        pop  1gh9q0HsS3tVXQypDXp4gf  4GGfAshSkqoxpZdoaHm7ky  \n",
       "7  artist_7        pop  4y3uAOMHISJ3OOdjPC1FFN  6e3pZKXUxrPfnUPJ960Hd9  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df=raw_df[raw_df['release_date'].notna() & raw_df['spotify_song_id'].notna() & raw_df['main_genre'].notna()]\n",
    "\n",
    "print(clean_df.shape) \n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bead54e",
   "metadata": {},
   "source": [
    "The genre and by extension rock_genre labels seem too specific to be useful. To confirm, check how many unique genres are listed and view some of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4192f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4113\n",
      "['technical deathcore', 'canadian metal', 'uk hip hop', 'euskal indie', 'indonesian jazz', 'slovenian pop', 'nouvelle chanson francaise', 'swedish soul', 'world', 'swedish indie pop', 'reggae mexicano', 'swedish emo', 'deep soft rock', 'cedm', 'deep german punk', 'swedish underground rap', 'beach house', 'garage punk blues', 'hmong pop', 'spanish new wave']\n"
     ]
    }
   ],
   "source": [
    "#find number of unique genres\n",
    "\n",
    "import random\n",
    "#since each genres entry is a list, split the list into components\n",
    "newlist=[string.split(\"' '\") for string in clean_df[clean_df.genres.notna()].genres]\n",
    "#unnest all the components into one list\n",
    "unnested_list=[j.strip(\"'\") for i in newlist for j in i]\n",
    "#analyze unique elements\n",
    "print(len(set(unnested_list)))\n",
    "print(random.sample(list(set(unnested_list)),20)) #random sample of unique genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eff4c5",
   "metadata": {},
   "source": [
    "There are 4992 genres and a quick glance shows many are quite specific. This information is likely too precise to be useful and will be dropped. By extension, also drop rock_genre feature. Finally, drop the 'id' feature because this has no useful information, and the artist_id features becasue the spotfiy_artist_id feature makes this redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "331e5e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chords</th>\n",
       "      <th>release_date</th>\n",
       "      <th>decade</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>main_genre</th>\n",
       "      <th>spotify_song_id</th>\n",
       "      <th>spotify_artist_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;intro_1&gt; E D A/Cs E D A/Cs &lt;verse_1&gt; E D A/Cs...</td>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>artist_2</td>\n",
       "      <td>metal</td>\n",
       "      <td>2ffJZ2r8HxI5DHcmf3BO6c</td>\n",
       "      <td>694QW15WkebjcrWgQHzRYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;intro_1&gt; Csmin &lt;verse_1&gt; A Csmin A Csmin A Cs...</td>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>artist_3</td>\n",
       "      <td>metal</td>\n",
       "      <td>5KiY8SZEnvCPyIEkFGRR3y</td>\n",
       "      <td>0niJkG4tKkne3zwr7I8n9n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;intro_1&gt; C &lt;verse_1&gt; G C G C &lt;chorus_1&gt; F Dmi...</td>\n",
       "      <td>2023-02-10</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>artist_5</td>\n",
       "      <td>pop</td>\n",
       "      <td>3zUecdrWC3IqrNSjhnoF3G</td>\n",
       "      <td>4GGfAshSkqoxpZdoaHm7ky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;intro_1&gt; G Bmin Amin D G Bmin &lt;verse_1&gt; Amin ...</td>\n",
       "      <td>2023-02-10</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>artist_5</td>\n",
       "      <td>pop</td>\n",
       "      <td>1gh9q0HsS3tVXQypDXp4gf</td>\n",
       "      <td>4GGfAshSkqoxpZdoaHm7ky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;intro_1&gt; Fsmin Fsno3d Bno3d E/B Fsno3d Bno3d ...</td>\n",
       "      <td>2023-02-10</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>artist_7</td>\n",
       "      <td>pop</td>\n",
       "      <td>4y3uAOMHISJ3OOdjPC1FFN</td>\n",
       "      <td>6e3pZKXUxrPfnUPJ960Hd9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              chords release_date  decade  \\\n",
       "1  <intro_1> E D A/Cs E D A/Cs <verse_1> E D A/Cs...   2003-01-01  2000.0   \n",
       "2  <intro_1> Csmin <verse_1> A Csmin A Csmin A Cs...   2003-01-01  2000.0   \n",
       "4  <intro_1> C <verse_1> G C G C <chorus_1> F Dmi...   2023-02-10  2020.0   \n",
       "6  <intro_1> G Bmin Amin D G Bmin <verse_1> Amin ...   2023-02-10  2020.0   \n",
       "7  <intro_1> Fsmin Fsno3d Bno3d E/B Fsno3d Bno3d ...   2023-02-10  2020.0   \n",
       "\n",
       "  artist_id main_genre         spotify_song_id       spotify_artist_id  \n",
       "1  artist_2      metal  2ffJZ2r8HxI5DHcmf3BO6c  694QW15WkebjcrWgQHzRYF  \n",
       "2  artist_3      metal  5KiY8SZEnvCPyIEkFGRR3y  0niJkG4tKkne3zwr7I8n9n  \n",
       "4  artist_5        pop  3zUecdrWC3IqrNSjhnoF3G  4GGfAshSkqoxpZdoaHm7ky  \n",
       "6  artist_5        pop  1gh9q0HsS3tVXQypDXp4gf  4GGfAshSkqoxpZdoaHm7ky  \n",
       "7  artist_7        pop  4y3uAOMHISJ3OOdjPC1FFN  6e3pZKXUxrPfnUPJ960Hd9  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df=clean_df.drop(columns=['id','genres','rock_genre'])\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ea318",
   "metadata": {},
   "source": [
    "If we take a quick look at the decade feature, we see that decades from 1890-1940 appear very sporadically, in fact these decades combined account for less than .07% of all data with a decade listed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc6461dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decade\n",
       "1890.0        23\n",
       "1900.0         1\n",
       "1920.0        40\n",
       "1930.0        58\n",
       "1940.0        83\n",
       "1950.0      1296\n",
       "1960.0      7916\n",
       "1970.0     13808\n",
       "1980.0     16491\n",
       "1990.0     33016\n",
       "2000.0     68686\n",
       "2010.0    128614\n",
       "2020.0     31532\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.decade.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3148231b",
   "metadata": {},
   "source": [
    "This class imbalance is far to large for these decades to be of any use, so we will restrict our study to decades from the 1950's on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b31061ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df=clean_df[(clean_df['decade']>1940) | clean_df['decade'].isna()==True]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b110fca3",
   "metadata": {},
   "source": [
    "Create a new feature 'chord_dict' by using Juan's song_split function to separate the song sections in the 'chords' feature into dictionary entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4dad8a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy over Juan's code\n",
    "\n",
    "import re\n",
    "\n",
    "# regex to capture tags like <verse_1>, <chorus_2>, <bridge>, etc.\n",
    "TAG = re.compile(r\"<\\s*([^>]+?)\\s*>\", flags=re.IGNORECASE)\n",
    "\n",
    "# Given string of chords partitioned into sections, returns dictionary of sections:chords in section.\n",
    "def song_split(chord_str: str):\n",
    "    s = (chord_str or \"\").strip()\n",
    "\n",
    "    # find all tags and their spans\n",
    "    spans = [(m.group(1).strip(), m.start(), m.end()) for m in TAG.finditer(s)]\n",
    "    if not spans:\n",
    "        return {\"whole\": s}  # no tags â†’ treat the whole thing as one section\n",
    "\n",
    "    # sentinel for the end of the string\n",
    "    spans.append((\"__END__\", len(s), len(s)))\n",
    "\n",
    "    chord_dict = {}\n",
    "    for (name, tag_start, tag_end), (_, next_start, _) in zip(spans, spans[1:]):\n",
    "        if name == \"__END__\":\n",
    "            break\n",
    "        # Get the segment between the end of the current tag and the start of the next tag\n",
    "        segment = s[tag_end:next_start].strip()\n",
    "        if segment:  # only keep non-empty segments\n",
    "            # If the section already exists, concatenate the new segment to the existing string\n",
    "            if name in chord_dict:\n",
    "                chord_dict[name] += \" \" + segment\n",
    "            else:\n",
    "                chord_dict[name] = segment\n",
    "\n",
    "    return chord_dict\n",
    "\n",
    "# make new 'chord_dict' feature\n",
    "clean_df.insert(1,'chord_dict',clean_df['chords'].apply(song_split))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7244af6d",
   "metadata": {},
   "source": [
    "Create some more basic features:\n",
    "* num_sections: number of sections the song is split into (verses, choruses, etc.)\n",
    "* tot_chords: the total number of chords listed in the progression\n",
    "* tot_unique_chords: number of unique chords appearing in the progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8855b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create num_sections\n",
    "clean_df.insert(2,'num_sections',clean_df['chord_dict'].apply(len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9431664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions to make tot_chords and tot_unique chords\n",
    "\n",
    "def total_chord_count(dict):\n",
    "    #split each sequence into a list\n",
    "    nest_list=[dict[i].split() for i in dict.keys()]\n",
    "    #concatenate lists\n",
    "    unnest_list=[j for i in nest_list for j in i]\n",
    "    return len(unnest_list)\n",
    "   \n",
    "def unique_chord_count(dict):\n",
    "    #split each sequence into a list\n",
    "    nest_list=[dict[i].split() for i in dict.keys()]\n",
    "    #concatenate lists\n",
    "    unnest_list=[j for i in nest_list for j in i]\n",
    "    return len(set(unnest_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f15fe3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tot_chords and total_unique chords\n",
    "clean_df.insert(3,'tot_chords',clean_df['chord_dict'].apply(total_chord_count))\n",
    "clean_df.insert(4,'tot_unique_chords',clean_df['chord_dict'].apply(unique_chord_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d26e65",
   "metadata": {},
   "source": [
    "Taking a quick peek at tot_chords, there are many entries that have only a few (<10) chord changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b5f7ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tot_chords\n",
       "1         84\n",
       "2         34\n",
       "3         69\n",
       "4        126\n",
       "5        332\n",
       "6        384\n",
       "7        431\n",
       "8        640\n",
       "9        545\n",
       "10       552\n",
       "11       608\n",
       "12       854\n",
       "13       833\n",
       "14       699\n",
       "15       923\n",
       "16      1195\n",
       "17      1016\n",
       "18      1058\n",
       "19      1201\n",
       "20      1236\n",
       "21      1389\n",
       "22      1261\n",
       "23      1346\n",
       "24      1630\n",
       "25      1789\n",
       "26      1403\n",
       "27      1652\n",
       "28      1892\n",
       "29      1744\n",
       "30      1710\n",
       "31      1982\n",
       "32      2124\n",
       "33      2189\n",
       "34      1998\n",
       "35      2007\n",
       "36      2438\n",
       "37      2438\n",
       "38      2184\n",
       "39      2322\n",
       "40      2676\n",
       "41      2627\n",
       "42      2421\n",
       "43      2670\n",
       "44      2784\n",
       "45      2810\n",
       "46      2601\n",
       "47      2609\n",
       "48      3260\n",
       "49      3142\n",
       "50      2847\n",
       "51      2933\n",
       "52      3279\n",
       "53      2978\n",
       "54      3076\n",
       "55      3183\n",
       "56      3504\n",
       "57      3368\n",
       "58      3047\n",
       "59      3062\n",
       "60      3694\n",
       "61      3481\n",
       "62      3058\n",
       "63      3163\n",
       "64      3713\n",
       "65      3336\n",
       "66      3301\n",
       "67      3319\n",
       "68      3484\n",
       "69      3487\n",
       "70      3267\n",
       "71      3128\n",
       "72      3826\n",
       "73      3294\n",
       "74      2950\n",
       "75      3260\n",
       "76      3493\n",
       "77      3098\n",
       "78      3122\n",
       "79      3022\n",
       "80      3451\n",
       "81      3192\n",
       "82      2925\n",
       "83      2808\n",
       "84      3311\n",
       "85      3043\n",
       "86      2766\n",
       "87      2795\n",
       "88      3075\n",
       "89      2658\n",
       "90      2755\n",
       "91      2669\n",
       "92      2879\n",
       "93      2600\n",
       "94      2433\n",
       "95      2333\n",
       "96      2802\n",
       "97      2351\n",
       "98      2205\n",
       "99      2186\n",
       "100     2344\n",
       "101     2092\n",
       "102     2077\n",
       "103     1996\n",
       "104     2169\n",
       "105     2059\n",
       "106     1809\n",
       "107     1667\n",
       "108     1918\n",
       "109     1712\n",
       "110     1671\n",
       "111     1567\n",
       "112     1802\n",
       "113     1557\n",
       "114     1555\n",
       "115     1497\n",
       "116     1470\n",
       "117     1333\n",
       "118     1336\n",
       "119     1310\n",
       "120     1455\n",
       "121     1211\n",
       "122     1173\n",
       "123     1099\n",
       "124     1152\n",
       "125     1029\n",
       "126     1024\n",
       "127      928\n",
       "128     1038\n",
       "129      972\n",
       "130      848\n",
       "131      760\n",
       "132      942\n",
       "133      833\n",
       "134      743\n",
       "135      710\n",
       "136      853\n",
       "137      678\n",
       "138      664\n",
       "139      565\n",
       "140      688\n",
       "141      578\n",
       "142      602\n",
       "143      539\n",
       "144      639\n",
       "145      565\n",
       "146      507\n",
       "147      473\n",
       "148      476\n",
       "149      415\n",
       "150      448\n",
       "151      400\n",
       "152      475\n",
       "153      404\n",
       "154      365\n",
       "155      330\n",
       "156      342\n",
       "157      328\n",
       "158      328\n",
       "159      321\n",
       "160      369\n",
       "161      282\n",
       "162      276\n",
       "163      270\n",
       "164      292\n",
       "165      240\n",
       "166      227\n",
       "167      226\n",
       "168      291\n",
       "169      231\n",
       "170      210\n",
       "171      185\n",
       "172      232\n",
       "173      166\n",
       "174      180\n",
       "175      174\n",
       "176      182\n",
       "177      182\n",
       "178      160\n",
       "179      129\n",
       "180      174\n",
       "181      131\n",
       "182      154\n",
       "183      142\n",
       "184      159\n",
       "185      130\n",
       "186      141\n",
       "187      105\n",
       "188      113\n",
       "189       94\n",
       "190       94\n",
       "191       96\n",
       "192      119\n",
       "193       90\n",
       "194       77\n",
       "195       87\n",
       "196       99\n",
       "197       77\n",
       "198       88\n",
       "199       65\n",
       "200       97\n",
       "201       69\n",
       "202       69\n",
       "203       66\n",
       "204       70\n",
       "205       73\n",
       "206       63\n",
       "207       56\n",
       "208       67\n",
       "209       50\n",
       "210       47\n",
       "211       46\n",
       "212       52\n",
       "213       55\n",
       "214       50\n",
       "215       44\n",
       "216       51\n",
       "217       47\n",
       "218       40\n",
       "219       34\n",
       "220       44\n",
       "221       36\n",
       "222       31\n",
       "223       35\n",
       "224       39\n",
       "225       41\n",
       "226       30\n",
       "227       27\n",
       "228       44\n",
       "229       36\n",
       "230       32\n",
       "231       30\n",
       "232       34\n",
       "233       26\n",
       "234       21\n",
       "235       22\n",
       "236       25\n",
       "237       33\n",
       "238       30\n",
       "239       23\n",
       "240       35\n",
       "241       18\n",
       "242       26\n",
       "243       11\n",
       "244       13\n",
       "245       11\n",
       "246       20\n",
       "247       16\n",
       "248       25\n",
       "249       15\n",
       "250       16\n",
       "251       12\n",
       "252       18\n",
       "253       13\n",
       "254       17\n",
       "255       17\n",
       "256       16\n",
       "257       18\n",
       "258       15\n",
       "259       12\n",
       "260       16\n",
       "261        8\n",
       "262       16\n",
       "263        9\n",
       "264       16\n",
       "265       19\n",
       "266       15\n",
       "267       12\n",
       "268       13\n",
       "269        8\n",
       "270       10\n",
       "271        8\n",
       "272        8\n",
       "273       16\n",
       "274        9\n",
       "275       14\n",
       "276       11\n",
       "277       10\n",
       "278        9\n",
       "279        7\n",
       "280       15\n",
       "281        5\n",
       "282        7\n",
       "283        6\n",
       "284        5\n",
       "285        8\n",
       "286        8\n",
       "287        8\n",
       "288        6\n",
       "289        5\n",
       "290        6\n",
       "291        7\n",
       "292        9\n",
       "293        3\n",
       "294       10\n",
       "295        2\n",
       "296        5\n",
       "297        9\n",
       "298       11\n",
       "299        3\n",
       "300        5\n",
       "301        3\n",
       "302        1\n",
       "303        5\n",
       "304        5\n",
       "305        3\n",
       "306        3\n",
       "308        6\n",
       "309        2\n",
       "310        6\n",
       "311        6\n",
       "312        6\n",
       "313        6\n",
       "314        4\n",
       "315        4\n",
       "316        8\n",
       "317        2\n",
       "318        1\n",
       "319        3\n",
       "320        4\n",
       "321        5\n",
       "322        4\n",
       "323        1\n",
       "324        5\n",
       "325        2\n",
       "326        1\n",
       "327        4\n",
       "328        3\n",
       "329        5\n",
       "330        1\n",
       "331        2\n",
       "332        3\n",
       "333        1\n",
       "335        1\n",
       "336        1\n",
       "337        1\n",
       "339        5\n",
       "340        3\n",
       "341        5\n",
       "342        1\n",
       "343        2\n",
       "344        3\n",
       "345        2\n",
       "346        3\n",
       "347        1\n",
       "348        1\n",
       "349        2\n",
       "350        2\n",
       "351        2\n",
       "352        1\n",
       "353        2\n",
       "354        2\n",
       "355        1\n",
       "356        3\n",
       "357        3\n",
       "358        2\n",
       "360        4\n",
       "361        2\n",
       "362        3\n",
       "363        1\n",
       "364        1\n",
       "365        1\n",
       "367        1\n",
       "368        2\n",
       "369        1\n",
       "370        2\n",
       "372        3\n",
       "374        5\n",
       "375        2\n",
       "376        1\n",
       "377        3\n",
       "380        1\n",
       "381        1\n",
       "383        1\n",
       "384        1\n",
       "386        1\n",
       "391        3\n",
       "392        2\n",
       "393        3\n",
       "394        2\n",
       "397        2\n",
       "399        1\n",
       "401        1\n",
       "402        2\n",
       "403        3\n",
       "405        2\n",
       "407        1\n",
       "408        2\n",
       "410        3\n",
       "411        1\n",
       "414        1\n",
       "416        2\n",
       "417        1\n",
       "418        1\n",
       "420        2\n",
       "421        2\n",
       "424        2\n",
       "431        1\n",
       "432        1\n",
       "433        1\n",
       "434        1\n",
       "440        2\n",
       "441        1\n",
       "443        2\n",
       "446        3\n",
       "448        1\n",
       "449        1\n",
       "454        2\n",
       "455        1\n",
       "456        1\n",
       "457        1\n",
       "461        1\n",
       "466        1\n",
       "474        1\n",
       "476        2\n",
       "484        1\n",
       "496        1\n",
       "497        1\n",
       "501        1\n",
       "502        1\n",
       "504        1\n",
       "513        1\n",
       "514        1\n",
       "515        1\n",
       "527        1\n",
       "534        1\n",
       "536        1\n",
       "537        1\n",
       "550        1\n",
       "561        1\n",
       "565        1\n",
       "573        2\n",
       "590        3\n",
       "603        1\n",
       "612        1\n",
       "618        1\n",
       "620        1\n",
       "630        1\n",
       "635        1\n",
       "641        1\n",
       "648        2\n",
       "681        1\n",
       "687        1\n",
       "709        1\n",
       "712        1\n",
       "723        1\n",
       "738        1\n",
       "748        1\n",
       "799        1\n",
       "807        1\n",
       "828        1\n",
       "843        1\n",
       "845        1\n",
       "862        1\n",
       "884        1\n",
       "903        1\n",
       "917        1\n",
       "921        1\n",
       "927        1\n",
       "933        1\n",
       "938        1\n",
       "954        1\n",
       "956        1\n",
       "970        1\n",
       "973        2\n",
       "990        1\n",
       "999        1\n",
       "1014       1\n",
       "1051       1\n",
       "1063       1\n",
       "1067       1\n",
       "1085       1\n",
       "1088       1\n",
       "1099       1\n",
       "1106       1\n",
       "1109       1\n",
       "1146       1\n",
       "1191       1\n",
       "1197       1\n",
       "1277       1\n",
       "1288       1\n",
       "1317       1\n",
       "1351       1\n",
       "1483       1\n",
       "1488       1\n",
       "1519       1\n",
       "1551       1\n",
       "1601       1\n",
       "1648       1\n",
       "2011       1\n",
       "2261       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.tot_chords.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a92d2a9",
   "metadata": {},
   "source": [
    "If we are using n-grams of up to length 5 as features, then we need to remove all data entries having 5 or fewer total chords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d685d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(301046, 11)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df=clean_df[clean_df['tot_chords']>5]\n",
    "clean_df=clean_df.reset_index(drop=True) #update row indices after removing data points\n",
    "clean_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b933c1",
   "metadata": {},
   "source": [
    "I naive measure of song complexity would be the ratio of the number of unique chords to the total number of chords. Add this feature to the data as unique_chord_density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86bba14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.insert(5,'unique_chord_density',clean_df['tot_unique_chords']/clean_df['tot_chords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40784e21",
   "metadata": {},
   "source": [
    "By extension, the number of unique 2,3,4 and 5-grams as as a ratio to the total number of chords could also be a complexity measure. Write a helper function to create new columns for unique n-gram density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fe15770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_n_density(dict,n):\n",
    "    #split each sequence into a list\n",
    "    nest_list=[dict[i].split() for i in dict.keys()]\n",
    "    \n",
    "    #concatenate lists\n",
    "    unnest_list=[j for i in nest_list for j in i]\n",
    "    unique=[]\n",
    "    no_unique=0\n",
    "    \n",
    "    #check sequential n-grams for uniqueness\n",
    "    for i in range(len(unnest_list)-n+1):\n",
    "        if unnest_list[i:i+n] not in unique:\n",
    "            unique.append(unnest_list[i:i+n])\n",
    "            no_unique+=1\n",
    "    \n",
    "    return(no_unique/len(unnest_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfa98d4",
   "metadata": {},
   "source": [
    "Add these features to the dataset: unique_2gram_density, unique_3gram_density, unique_4gram_density, unique_5gram_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e8889a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.insert(6,'unique_2gram_density',clean_df['chord_dict'].apply(unique_n_density,n=2))\n",
    "clean_df.insert(7,'unique_3gram_density',clean_df['chord_dict'].apply(unique_n_density,n=3))\n",
    "clean_df.insert(8,'unique_4gram_density',clean_df['chord_dict'].apply(unique_n_density,n=4))\n",
    "clean_df.insert(9,'unique_5gram_density',clean_df['chord_dict'].apply(unique_n_density,n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43507624",
   "metadata": {},
   "source": [
    "Another useful measure of song complexity could be the difference between the number of unique 2-grams and number of unique chords, normalized by the total number of chords. This is because a song which repeats the same progression over and over will have the same number of unique chords and number of unique 2-grams. For example, a song that repeats I-IV-V-I over and over has three unique chords (I,IV,V) and three unique 2-grams ([I,IV],[IV,V],[V,I]). If the motif changes or the key modulates, the difference should capture this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4dbd2a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_diff=(clean_df['unique_2gram_density']-clean_df['unique_chord_density'])\n",
    "clean_df.insert(10,'unique_2gram_difference',unique_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce672b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv('../../data/clean.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
