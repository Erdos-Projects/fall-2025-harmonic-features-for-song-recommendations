{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-28T21:24:28.337131Z",
     "start_time": "2025-10-28T21:24:28.053808Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:24:28.341632Z",
     "start_time": "2025-10-28T21:24:28.339999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_folder_path = Path('../../data/')\n",
    "print(os.listdir(data_folder_path))"
   ],
   "id": "8d7615199183047a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data_train_all_columns.csv', 'chordonomicon_raw.csv', 'final_test.csv', 'matrix_expanded_final_train.csv', 'density_expanded_final_train.csv', '.DS_Store', 'spotify_hot100.csv', 'data_train_continuous_data.csv', 'spotify_final.csv', 'clean_test_with_4_grams.csv', 'final_train.csv', 'final_train_pop.csv', 'data_train_n_grams.csv', 'data_train_all.csv', 'clean_test_with_5_grams.csv', 'clean_test_with_3_grams.csv', 'final_test_pop.csv']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:24:31.725944Z",
     "start_time": "2025-10-28T21:24:28.343964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_continuous = pd.read_csv(data_folder_path / 'data_train_continuous_data.csv')\n",
    "print(len(data_continuous))"
   ],
   "id": "54083472dc985983",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255606\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:24:33.206559Z",
     "start_time": "2025-10-28T21:24:31.778978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_n_grams = pd.read_csv(data_folder_path / 'data_train_n_grams.csv')\n",
    "print(len(data_n_grams))"
   ],
   "id": "ad42e6db73182502",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255606\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:24:33.726806Z",
     "start_time": "2025-10-28T21:24:33.229792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define common columns that should be merged on\n",
    "common_cols = ['spotify_song_id', 'chords', 'simplified_chords', 'decade', 'main_genre']\n",
    "\n",
    "merged_data = data_continuous.merge(data_n_grams, on=common_cols, how='inner')\n",
    "\n",
    "print(f\"Merged data shape: {merged_data.shape}\")\n",
    "print(f\"Columns: {len(merged_data.columns)}\")"
   ],
   "id": "8d9a91d2daaba002",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data shape: (255606, 142)\n",
      "Columns: 142\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:24:33.817867Z",
     "start_time": "2025-10-28T21:24:33.732819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if they have the same values\n",
    "are_same = merged_data['spotify_song_id'].equals(merged_data['spotify_track_id'])\n",
    "print(f\"Are spotify_song_id and spotify_track_id the same? {are_same}\")\n",
    "\n",
    "# Check for any differences\n",
    "if not are_same:\n",
    "    differences = merged_data[merged_data['spotify_song_id'] != merged_data['spotify_track_id']]\n",
    "    print(f\"\\nNumber of rows with differences: {len(differences)}\")\n",
    "    print(f\"Total rows: {len(merged_data)}\")\n",
    "\n",
    "# Check if all values match\n",
    "matching_count = (merged_data['spotify_song_id'] == merged_data['spotify_track_id']).sum()\n",
    "print(f\"\\nMatching values: {matching_count}/{len(merged_data)}\")\n",
    "\n",
    "print(f\"\\nThe 'spotify_track_id' column contains nans for rows that do not have Spotify data\")"
   ],
   "id": "d4e08df8b6077b8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are spotify_song_id and spotify_track_id the same? False\n",
      "\n",
      "Number of rows with differences: 73498\n",
      "Total rows: 255606\n",
      "\n",
      "Matching values: 182108/255606\n",
      "\n",
      "The 'spotify_track_id' column contains nans for rows that do not have Spotify data\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:24:33.828298Z",
     "start_time": "2025-10-28T21:24:33.822589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a DataFrame with column names\n",
    "columns_df = pd.DataFrame({'column_name': merged_data.columns})\n",
    "\n",
    "# Define your column lists\n",
    "id_cols = ['spotify_song_id', 'spotify_track_id']\n",
    "data_cols = ['chords_x', 'simplified_chords_x', 'simplified_chords', 'spotify_song_id', 'chords_y', 'missing_notes', 'simplified_chords_y', 'chords', 'spotify_artist_id', 'spotify_success', 'spotify_duration_ms', 'spotify_artist_popularity', 'spotify_followers', 'spotify_track_name', 'spotify_artists', 'spotify_album_name', 'spotify_release_date', 'spotify_artist_name', 'spotify_genres']\n",
    "\n",
    "target_cols = ['decade', 'main_genre', 'spotify_popularity']\n",
    "predictor_continuous = ['drone_ratio', 'average_overlap', 'average_2overlap', 'average_3overlap', 'average_4overlap', 'average_5overlap', 'maj_triad_ratio', 'min_triad_ratio', 'unique_5gram_density', 'unique_chord_density']\n",
    "\n",
    "# Create function to categorize columns\n",
    "def categorize_column(col):\n",
    "    if col in id_cols:\n",
    "        return 'id'\n",
    "    elif col in data_cols:\n",
    "        return 'data'\n",
    "    elif col in target_cols:\n",
    "        return 'target'\n",
    "    elif col in predictor_continuous:\n",
    "        return 'predictor_continuous'\n",
    "    elif col.startswith('contain'):\n",
    "        return 'predictor_n_grams'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "# Create DataFrame with column names and types\n",
    "columns_df = pd.DataFrame({\n",
    "    'column_name': merged_data.columns,\n",
    "    'column_type': [categorize_column(col) for col in merged_data.columns]\n",
    "})\n",
    "\n",
    "print(columns_df['column_type'].value_counts())\n",
    "print(f\"\\nTotal columns: {len(columns_df)}\")\n"
   ],
   "id": "c8e2ad98966535b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_type\n",
      "predictor_n_grams       109\n",
      "data                     18\n",
      "predictor_continuous     10\n",
      "target                    3\n",
      "id                        2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total columns: 142\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:24:34.709968Z",
     "start_time": "2025-10-28T21:24:33.835619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add the hot 100 index to the data\n",
    "\n",
    "data_hot_100 = pd.read_csv(data_folder_path / 'spotify_hot100.csv')\n",
    "data_hot_100 = pd.read_csv(data_folder_path / 'spotify_hot100.csv', usecols=['track_id', 'on_hot100'])\n",
    "\n",
    "print(data_hot_100.columns)\n",
    "\n",
    "merged_data_final = merged_data.merge(\n",
    "    data_hot_100,\n",
    "    how='left',\n",
    "    left_on='spotify_track_id',\n",
    "    right_on='track_id'\n",
    ")\n",
    "print(len(merged_data_final))\n",
    "\n",
    "# add to column csv\n",
    "new_row = {'column_name': 'track_id', 'column_type': 'id'}\n",
    "columns_df = pd.concat([columns_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "new_row = {'column_name': 'on_hot100', 'column_type': 'target'}\n",
    "columns_df = pd.concat([columns_df, pd.DataFrame([new_row])], ignore_index=True)"
   ],
   "id": "6e90d3abac6a8483",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['track_id', 'on_hot100'], dtype='object')\n",
      "255606\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:24:34.735580Z",
     "start_time": "2025-10-28T21:24:34.732330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the desired order of column types\n",
    "type_order = ['id', 'data', 'target', 'predictor_continuous', 'predictor_n_grams']\n",
    "\n",
    "# Create a categorical type with the specified order\n",
    "columns_df['column_type'] = pd.Categorical(columns_df['column_type'], categories=type_order, ordered=True)\n",
    "\n",
    "# Sort by column_type\n",
    "columns_df = columns_df.sort_values('column_type').reset_index(drop=True)"
   ],
   "id": "1a41906d24c167fe",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:24:34.743642Z",
     "start_time": "2025-10-28T21:24:34.739614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save to CSV\n",
    "output_path = data_folder_path / 'data_train_all_columns.csv'\n",
    "columns_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved {len(merged_data.columns)} column names to {output_path}\")"
   ],
   "id": "150a52635b323e91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 142 column names to ../../data/data_train_all_columns.csv\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:25:37.180362Z",
     "start_time": "2025-10-28T21:25:28.068469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save merged data to CSV\n",
    "output_path = data_folder_path / 'data_train_all.csv'\n",
    "merged_data_final.to_csv(output_path, index=False, na_rep='NaN')\n",
    "print(f\"Saved merged data to {output_path}\")"
   ],
   "id": "c7736cdfe2fd8e7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged data to ../../data/data_train_all.csv\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
